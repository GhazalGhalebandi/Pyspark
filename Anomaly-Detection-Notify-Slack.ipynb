{"cells":[{"cell_type":"code","source":["# This notebook compute anomaly for max_apply_date -1 day with past 90 days window\n# Terminology:\n# {}_thresholds: the actual value of 5th and 95th percentiles, any value outside of the range considered anomaly\n# {}_score: the mean score of the compute date (overall/channel: max_apply_date -1 day or yesterday; positive/negative: max_apply_date -1 -21, with the assumption of hirer take 21 days to update application status)\n# {}_trigger: trigger as anomaly? 1 yes 0 no\n# {}_count: the datapoints or number of applications from the compute date (overall/channel: max_apply_date -1 day or yesterday; positive/negative: max_apply_date -1 -21, with the assumption of hirer take 21 days to update application status)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%run ./library"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dbutils.widgets.text(\"country\", \"my\", \"country\")\ndbutils.widgets.text(\"max_app_id\", \"\", \"max_app_id\")\ncountry = dbutils.widgets.get(\"country\")\nmax_app_id = int(dbutils.widgets.get(\"max_app_id\"))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["config = dbutils.notebook.run(\"config\", 0, {\"country\": country})\nparams = json.loads(config)\n\napplication_status_window = params['application_status_window']\napplication_signal_window = params['application_signal_window']\nmin_percentile = params['min_percentile']\nmax_percentile = params['max_percentile']\n\n# conversion_function_window = params['conversion_function_window']\ndescriptive_fields = params['meta_tables']['descriptive_fields']\napplication_signals = params['meta_tables']['application_signals']\ndata_mart_predictions = params['common_dirs']['data_mart_predictions']\nanomaly_result_table = params['meta_tables']['anomaly_detection_summary']\n"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["#checking the dependency\nassert prerequisite_check(max_app_id, [JOB_PLACEMENTS_PROXY_EXTRACTION, JOB_PLACEMENTS_PROXY_SCORE, JOB_PLACEMENTS_PROXY_DESCRIPTIVE, JOB_PLACEMENTS_PROXY_SIGNALS], params[\"meta_tables\"][\"state_management\"])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["prediction = spark.read.parquet(data_mart_predictions)\ndescriptive = spark.table(descriptive_fields)\nsignal = spark.table(application_signals)\n\n# join tables and derive max_date\nsignal = drop_ordered_duplicates(signal, 'application_id', 'action_date')\ndescriptive = descriptive.select(\n  'application_id', \n  col('apply_date').substr(0,10).alias('apply_date'),\n  'application_channel_code', \n  'application_channel_desc'\n)\n\ndataset = prediction.join(\n  descriptive,\n  'application_id'\n).join(\n  signal,\n  'application_id'\n)\n\nmax_apply_date = dataset.select(max('apply_date').alias('max_apply_date')).collect()[0]['max_apply_date']"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["yesterday = (datetime.strptime(max_apply_date,\"%Y-%m-%d\") - timedelta(days = 1)).strftime(\"%Y-%m-%d\")\nwindow_start_date = (datetime.strptime(yesterday,\"%Y-%m-%d\") - timedelta(days = int(application_status_window))).strftime(\"%Y-%m-%d\")\nwindow_signal_end_date =  (datetime.strptime(yesterday,\"%Y-%m-%d\") - timedelta(days = int(application_signal_window))).strftime(\"%Y-%m-%d\")"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["dataset = dataset.filter(\n  (col('apply_date') > window_start_date)\n).withColumn(\n  'label', label_application_status('application_status_code')\n)\n\ndataset.cache()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["MODE_OVERALL = \"OVERALL\"\nMODE_POSITIVE = \"POSITIVE\"\nMODE_NEGATIVE = \"NEGATIVE\"\nMODE_CHANNEL = \"CHANNEL\"\n\ndef anomaly_detection(df, mode, channel=None):\n  \n  # get daily mean\n  daily_mean_df = None\n  \n  if mode == MODE_NEGATIVE:\n    df = df.filter(\n      (col('label').isin([0])) &\n      (col('apply_date') <= window_signal_end_date)\n    )\n  elif mode == MODE_POSITIVE:\n     df = df.filter(\n      (col('label').isin([1])) &\n      (col('apply_date') <= window_signal_end_date)\n    )\n  elif mode == MODE_OVERALL:\n    df = df.filter(\n      (col('label').isin([0, 1, -1]))\n    )\n  elif mode == MODE_CHANNEL:\n    df = df.filter(\n      col('application_channel_desc') == channel\n    )\n  \n  df_count = None\n  thresholds = [None, None]\n  today_score = None\n  trigger = None\n  \n# skip anomaly detection if df.count()/datapoints lesser than 11\n# default None value to anomaly_detection_summary (lower_threshold, upper_threshold, score, num_app) columns \n  if df.count() > 11:\n    \n    # get count()\n    df_count = df.filter(\n      col('apply_date') == (yesterday if mode in [MODE_OVERALL, MODE_CHANNEL] else window_signal_end_date)\n    ).count()\n\n    # get daily mean\n    daily_mean_df = df.groupby(\n      'apply_date'\n    ).agg(\n      mean('score').alias('mean_score')\n    )\n\n    # calculate threshold\n    thresholds = make_threshold(\n      daily_mean_df,\n      yesterday,\n      min_percentile,\n      max_percentile\n    )\n\n    # get today's score\n    today_score = daily_mean_df.filter(\n      col('apply_date') == (yesterday if mode in [MODE_OVERALL, MODE_CHANNEL] else window_signal_end_date)\n    ).select(\n      'mean_score'\n    ).collect()[0][0]\n\n    # decide trigger\n    trigger = anomaly_detector(\n      today_score,\n      thresholds\n    )\n \n  return thresholds, today_score, trigger, df_count"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["overall_thresholds, overall_score, overall_trigger, overall_count = anomaly_detection(dataset, MODE_OVERALL)\npositive_thresholds, positive_score, positive_trigger, positive_count = anomaly_detection(dataset, MODE_POSITIVE)\nnegative_thresholds, negative_score, negative_trigger, negative_count = anomaly_detection(dataset, MODE_NEGATIVE)\nanomaly_list = []\nanomaly_label_list = []\n\n# send alerts to slack channel\nif overall_trigger == 1:\n  anomaly_list.append('overall_mean')\nif positive_trigger == 1:\n  anomaly_label_list.append('positive_label_mean')\nif negative_trigger == 1:\n  anomaly_label_list.append('negative_label_mean')"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["print(overall_thresholds, overall_score, overall_trigger, overall_count)\nprint(positive_thresholds, positive_score, positive_trigger, positive_count)\nprint(negative_thresholds, negative_score, negative_trigger, negative_count)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql use placements_proxy;\ncreate table if not exists placements_proxy.anomaly_detection_summary (\n  tag STRING COMMENT 'types of scores', \n  run_date TIMESTAMP COMMENT 'the date that notebook is run', \n  date STRING,\n  lower_threshold DOUBLE,\n  upper_threshold DOUBLE,\n  score DOUBLE COMMENT 'scores for the date',\n  num_app INT COMMENT 'number of application for date',\n  is_anomaly BOOLEAN,\n  country STRING,\n  remarks STRING\n) USING PARQUET\npartitioned by (\n  country, \n  date\n);\n\ncreate table if not exists placements_proxy_dev.anomaly_detection_summary (\n  tag STRING COMMENT 'types of scores', \n  run_date TIMESTAMP COMMENT 'the date that notebook is run', \n  date STRING,\n  lower_threshold DOUBLE,\n  upper_threshold DOUBLE,\n  score DOUBLE COMMENT 'scores for the date',\n  num_app INT COMMENT 'number of application for date',\n  is_anomaly BOOLEAN,\n  country STRING,\n  remarks STRING\n) USING PARQUET\npartitioned by (\n  country, \n  date\n);"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Create a database so to link with Tableau\nrun_date = datetime.now()\noverall_mean_row = anomaly_table_row(\n  'overall_mean', \n  run_date, \n  yesterday, \n  overall_thresholds, \n  overall_score, \n  overall_count, \n  overall_trigger, \n  country, \n  window_start_date, \n  yesterday\n)\n\nnegative_label_mean_row = anomaly_table_row(\n  'negative_label_mean', \n  run_date, \n  window_signal_end_date, \n  negative_thresholds, \n  negative_score, \n  negative_count, \n  negative_trigger, \n  country, \n  window_start_date, \n  window_signal_end_date\n)\n\npositive_label_mean_row = anomaly_table_row(\n  'positive_label_mean', \n  run_date, \n  window_signal_end_date, \n  positive_thresholds, \n  positive_score, \n  positive_count, \n  positive_trigger, \n  country, \n  window_start_date, \n  window_signal_end_date\n)\n\nschema = anomaly_table_schema()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["row = overall_mean_row + positive_label_mean_row + negative_label_mean_row"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["result = spark.createDataFrame(row, schema)\nresult.write.saveAsTable(anomaly_result_table, mode = 'append', partitionBy = ['country','date'])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["#####Application Channel Anomaly Detection"],"metadata":{}},{"cell_type":"code","source":["dataset_patched = dataset.select(\n  'apply_date'\n).distinct().withColumn('flag', lit(0)).join(\n  dataset.select(\n    'application_channel_code'\n  ).distinct().withColumn('flag', lit(0)), \n  on = ['flag']\n).join(\n  dataset, \n  on = ['apply_date','application_channel_code'], \n  how = 'left_outer'\n).fillna(0, subset = ['score'])"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["dataset_patched.cache()\ndataset_patched.count()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["channel_list = dataset_patched.filter(\n  col('apply_date') == yesterday\n).select(\n  'application_channel_desc'\n).dropna().distinct().collect()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["for i in range(len(channel_list)):\n  channel_name = channel_list[i].application_channel_desc\n  channel_thresholds, channel_mean_score, trigger_channel, channel_number_of_application = anomaly_detection(\n    dataset_patched, \n    mode=MODE_CHANNEL, \n    channel=channel_name\n  )\n\n  if trigger_channel == 1:\n    anomaly_list.append('channel: ' + channel_name)\n    \n      \n  application_channel_mean_row = anomaly_table_row(\n    channel_name,\n    run_date,\n    yesterday,\n    channel_thresholds,\n    channel_mean_score,\n    channel_number_of_application,\n    trigger_channel,\n    country,\n    window_start_date,\n    yesterday\n  )\n  \n  result = spark.createDataFrame(application_channel_mean_row, schema)\n  result.write.saveAsTable(anomaly_result_table, mode = 'append', partitionBy = ['country','date'])"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["if len(anomaly_list) != 0:\n  slack_anomaly_message(anomaly_list, yesterday, country)\n\nif len(anomaly_label_list) != 0:\n  slack_anomaly_message(anomaly_label_list, window_signal_end_date, country)"],"metadata":{},"outputs":[],"execution_count":21}],"metadata":{"name":"anomaly-detection-tdigest","notebookId":1047791},"nbformat":4,"nbformat_minor":0}
