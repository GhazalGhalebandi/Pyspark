{"cells":[{"cell_type":"markdown","source":["### Predicting number of dengi cases \n - changed the number of iterations"],"metadata":{}},{"cell_type":"code","source":["from pyspark import keyword_only\nfrom pyspark.ml import *\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import *\nfrom pyspark.ml.classification import *\nfrom pyspark.ml.clustering import *\nfrom pyspark.ml.evaluation import *\nfrom pyspark.ml.tuning import *\nfrom pyspark.ml.param.shared import *\nfrom pyspark.ml.param import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom HTMLParser import HTMLParser\nfrom math import sqrt\nfrom math import isnan\nfrom datetime import datetime\nimport numpy\nimport re\nimport random"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["dengue_features_train_schema = StructType([\n  StructField('city', StringType(), True),\n  StructField('year', FloatType(), True),\n  StructField('weekofyear', FloatType(), True),\n  StructField('week_start_date', DateType(), True),\n  StructField('ndvi_ne', FloatType(), True),\n  StructField('ndvi_nw', FloatType(), True),\n  StructField('ndvi_se', FloatType(), True),\n  StructField('ndvi_sw', FloatType(), True),\n  StructField('precipitation_amt_mm', FloatType(), True),\n  StructField('reanalysis_air_temp_k', FloatType(), True),\n  StructField('reanalysis_avg_temp_k', FloatType(), True),\n  StructField('reanalysis_dew_point_temp_k', FloatType(), True),\n  StructField('reanalysis_max_air_temp_k', FloatType(), True),\n  StructField('reanalysis_min_air_temp_k', FloatType(), True),\n  StructField('reanalysis_precip_amt_kg_per_m2', FloatType(), True),\n  StructField('reanalysis_relative_humidity_percent', FloatType(), True),\n  StructField('reanalysis_sat_precip_amt_mm', FloatType(), True),\n  StructField('reanalysis_specific_humidity_g_per_kg', FloatType(), True),\n  StructField('reanalysis_tdtr_k', FloatType(), True),\n  StructField('station_avg_temp_c', FloatType(), True),\n  StructField('station_diur_temp_rng_c', FloatType(), True),\n  StructField('station_max_temp_c', FloatType(), True),\n  StructField('station_min_temp_c', FloatType(), True),\n  StructField('station_precip_mm', FloatType(), True)\n])\n\ndengue_labels_train_schema = StructType([\n  StructField('city', StringType(), True),\n  StructField('year', FloatType(), True),\n  StructField('weekofyear', FloatType(), True),\n  StructField('total_cases', FloatType(), True)\n])\n\n"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["train = spark.read.schema(dengue_features_train_schema).csv(\"s3a://data/dengai/dengue_features_train.csv\" , header = True)\nlabel_dataset = spark.read.schema(dengue_labels_train_schema).csv('s3a://data/dengai/dengue_labels_train.csv', header = True)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["train_labels = train.join(label_dataset , ['city' , 'year' , 'weekofyear'])"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["train_labels.select('city').distinct().show()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["def numerical_cities(city):\n  if city == 'iq':\n    return '1'\n  else:\n    return '2'\n  \ntransform_city = udf ( numerical_cities , StringType())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["train_labels_transform_city = train_labels.withColumn('num_city', transform_city('city'))\ntrain_labels_transform_city = train_labels_transform_city.drop('city')\ndisplay(train_labels_transform_city)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["train_labels_transform_city_year = train_labels_transform_city.withColumn('year', substring(col('week_start_date'),0,4))\ntrain_labels_transform_city_year_month = train_labels_transform_city_year.withColumn('month', substring(col('week_start_date'), 6,2))\ntrain_labels_transform_city_year_month_day = train_labels_transform_city_year_month.withColumn('day', substring(col('week_start_date'), 9,2))\n\ntrain_clean = train_labels_transform_city_year_month_day.drop('week_start_date')\ndisplay(train_clean)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["def fill_with_mean(df, exclude=set()): \n    stats = df.agg(*(\n        avg(c).alias(c) for c in df.columns if c not in exclude\n    ))\n    return df.na.fill(stats.first().asDict())\n\ntrain_clean_nonull = fill_with_mean(train_clean, [\"year\", \"month\", \"day\" , \"num_city\"])"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["describe_mainTrain_df = train_clean_nonull.describe()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Normalization \n  - x = value \n  - dl = min of attribute \n  - dh = max of attribute \n  - nl = min of expected range \n  - nh = max of expected range"],"metadata":{}},{"cell_type":"code","source":["mainTrain = train_clean_nonull"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# call function\n#normalize columns\ndef normalizing_column_1(c , dL, dH):\n  nL = 0\n  nH = 1\n  numi = (float(c) - dL) * (nH-nL)\n  denom = dH - dL\n  div = float(numi) / float(denom)\n  normalized = float(div + nL)\n  return normalized\n\nnormalizing_column = udf(normalizing_column_1, DoubleType())\n\n\nnames = mainTrain.schema.names\nfor colname in names:\n  dL = float(describe_mainTrain_df.collect()[3][colname])\n  dH = float(describe_mainTrain_df.collect()[4][colname])\n  mainTrain = mainTrain.withColumn('normalized_' + str(colname), \n                           normalizing_column(colname, lit(dL) , lit(dH))\n                          )                                                                   \n    "],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["normalized_mainTrain = spark.read.parquet(\"s3a://dengi-ghazalg/normalized_mainTrain2_24NOV2017\")\ndisplay(normalized_mainTrain)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["normalized_mainTest = spark.read.parquet(\"s3a://dengi-ghazalg/normalized_mainTest2_24NOV2017\")\ndisplay(normalized_mainTest)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["normalized_mainTest.select('normalized_num_city').groupby('normalized_num_city').count().show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Tensor Flow"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["normalized_mainTrain_pd = normalized_mainTrain.toPandas()\nnormalized_mainTest_pd = normalized_mainTest.toPandas()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["normalized_mainTest_pd.year = normalized_mainTest_pd.year.astype(float)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["non_feature_columns = ['year' , 'weekofyear']"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["feature_columns = [\n  'normalized_year', \n  'normalized_weekofyear', \n  'normalized_ndvi_ne', \n  'normalized_ndvi_nw',\n  'normalized_ndvi_se', \n  'normalized_ndvi_sw', \n  'normalized_precipitation_amt_mm',\n  'normalized_reanalysis_air_temp_k', \n  'normalized_reanalysis_avg_temp_k',\n  'normalized_reanalysis_dew_point_temp_k', \n  'normalized_reanalysis_max_air_temp_k',\n  'normalized_reanalysis_min_air_temp_k',\n  'normalized_reanalysis_precip_amt_kg_per_m2',\n  'normalized_reanalysis_relative_humidity_percent',\n  'normalized_reanalysis_sat_precip_amt_mm',\n  'normalized_reanalysis_specific_humidity_g_per_kg', \n  'normalized_reanalysis_tdtr_k',\n  'normalized_station_avg_temp_c', \n  'normalized_station_diur_temp_rng_c',\n  'normalized_station_max_temp_c', \n  'normalized_station_min_temp_c',\n  'normalized_station_precip_mm', \n  'normalized_num_city',\n  'normalized_month',\n  'normalized_day'\n]\n\nlabel_columns = ['total_cases']"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["import tensorflow as tf\n\ntf.reset_default_graph()\n\nfeature_columns_tf = [\n  tf.feature_column.numeric_column(\"normalized_year\"), \n  tf.feature_column.numeric_column(\"normalized_weekofyear\"), \n  tf.feature_column.numeric_column(\"normalized_ndvi_ne\"), \n  tf.feature_column.numeric_column(\"normalized_ndvi_nw\"),\n  tf.feature_column.numeric_column(\"normalized_ndvi_se\"), \n  tf.feature_column.numeric_column(\"normalized_ndvi_sw\"), \n  tf.feature_column.numeric_column(\"normalized_precipitation_amt_mm\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_air_temp_k\"), \n  tf.feature_column.numeric_column(\"normalized_reanalysis_avg_temp_k\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_dew_point_temp_k\"), \n  tf.feature_column.numeric_column(\"normalized_reanalysis_max_air_temp_k\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_min_air_temp_k\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_precip_amt_kg_per_m2\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_relative_humidity_percent\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_sat_precip_amt_mm\"),\n  tf.feature_column.numeric_column(\"normalized_reanalysis_specific_humidity_g_per_kg\"), \n  tf.feature_column.numeric_column(\"normalized_reanalysis_tdtr_k\"),\n  tf.feature_column.numeric_column(\"normalized_station_avg_temp_c\"), \n  tf.feature_column.numeric_column(\"normalized_station_diur_temp_rng_c\"),\n  tf.feature_column.numeric_column(\"normalized_station_max_temp_c\"), \n  tf.feature_column.numeric_column(\"normalized_station_min_temp_c\"),\n  tf.feature_column.numeric_column(\"normalized_station_precip_mm\"), \n  tf.feature_column.numeric_column(\"normalized_num_city\"),\n  tf.feature_column.numeric_column(\"normalized_month\"),\n  tf.feature_column.numeric_column(\"normalized_day\")\n]\n\n# Define the train inputs\ntrain_input_fn = tf.estimator.inputs.pandas_input_fn(\n    x = normalized_mainTrain_pd[feature_columns],\n    y = normalized_mainTrain_pd[label_columns],\n    num_epochs=None,\n    shuffle=True)\n\n\n\n\n# Define train inputs for evaluation\ntrain_input_fn_eval = tf.estimator.inputs.pandas_input_fn(\n    x = normalized_mainTrain_pd[feature_columns],\n    y = normalized_mainTrain_pd[label_columns],\n    num_epochs = 1,\n    shuffle = False)\n\n\n# Define test inputs for evaluation\ntest_input_fn_eval = tf.estimator.inputs.pandas_input_fn(\n    x = normalized_mainTest_pd[feature_columns],\n    y = None,\n    num_epochs=1,\n    shuffle=False)\n\n\n\nclassifier = tf.estimator.DNNRegressor(\n  feature_columns = feature_columns_tf,\n  hidden_units = [200, 300, 200, 300, 200, 300],\n  optimizer = tf.train.ProximalAdagradOptimizer(\n    learning_rate=0.01,\n    l1_regularization_strength=0.001\n  ),\n  activation_fn = tf.nn.relu,\n  model_dir= \"/tmp/tf_dengai_mainTrain_ghazall_24Nov_v3\"\n)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Train model.\nclassifier.train(input_fn = train_input_fn, steps=50000)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Evaluate accuracy.\naverage_loss_train = classifier.evaluate(input_fn = train_input_fn_eval)[\"average_loss\"]\nprint(\"Train Average Loss: {0:f}\\n\".format(average_loss_train))\n\n# INFO:tensorflow:Saving dict for global step 50000: average_loss = 0.210588, global_step = 50000, loss = 25.5514\n# Train Average Loss: 0.210588"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Evaluate accuracy.\naverage_loss_test = classifier.predict(input_fn = test_input_fn_eval)\n#print(\"Test Average Loss: {0:f}\\n\".format(average_loss_test))"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["predictions_list = [prediction['predictions'][0] for prediction in average_loss_test]\nlen(predictions_list)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["print predictions_list"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["list_to_df = pd.DataFrame({'predictions':predictions_list})\nprint (list_to_df)"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["cons = normalized_mainTest_pd[feature_columns].join(list_to_df)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["spark.createDataFrame(\n    cons\n  ).count()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["import math"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["def roundup(value):\n  return math.ceil(value)\n\nroundup = udf(roundup,FloatType())\n\ndef str_city(city):\n  if(city == 1):\n    return 'sj'\n  elif(city == 2):\n    return 'iq'\n\nstr_city = udf(str_city,StringType())"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["normalized_mainTest_pd.year = normalized_mainTest_pd.year.astype(float)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["normalized_mainTest_pd_selected = normalized_mainTest_pd[['year', 'weekofyear']]\n"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["normalized_mainTest_pd_selected"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["result = pd.concat([cons, normalized_mainTest_pd_selected], axis=1)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["pd_to_spark = spark.createDataFrame(result)\ndisplay(pd_to_spark)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["new_df = pd_to_spark.select(\n   col('normalized_num_city').alias('city'),  \n   col('year'),\n   col('weekofyear'),\n   col('predictions').alias(\"total_cases\")\n)\n\ndisplay(new_df)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["new_df.select('city').distinct().show()"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["def str_city(city):\n  if(city == 1):\n    return 'sj'\n  elif(city == 0):\n    return 'iq'\n\nstr_city = udf(str_city,StringType())"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["final_df3 = new_df.withColumn(\"city2\", str_city('city')).drop('city')"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["display(final_df3)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["submission_df12 = final_df3.select(\n  col('city2').alias('city'),  \n  col('year'),\n  col('weekofyear'),\n  col('total_cases')\n)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["display(submission_df12)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["submission_df12.printSchema()"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":[" import math"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["def roundup(value):\n  return math.ceil(value)\n\nroundup = udf(roundup,FloatType())\nsubmission_df12 = submission_df12.withColumn('total_cases2', roundup('total_cases'))\n"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["display(submission_df12)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["submission_df6 = submission_df12.drop('total_cases')\ndisplay(submission_df6)"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["submission_df7 = submission_df6.select(\n  col('city'), \n  col('year'), \n  col('weekofyear'), \n  col('total_cases2').alias('total_cases')\n)"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["display(submission_df7)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["submission_df7.printSchema()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":55}],"metadata":{"name":"Dengue-DNNRegressor-Final","notebookId":1408661},"nbformat":4,"nbformat_minor":0}
